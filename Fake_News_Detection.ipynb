{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6N+9uFSVdpPVitGs6j28Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KALYAN8825/M--PROJECT-YBI-/blob/main/Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZlRbdkteiFd",
        "outputId": "2842348a-a7d4-4964-eb3c-7ea1a47055b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.8.0/de_core_news_lg-3.8.0-py3-none-any.whl (567.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.8/567.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-lg\n",
            "Successfully installed de-core-news-lg-3.8.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.8.0/de_core_news_lg-3.8.0-py3-none-any.whl\n",
        "!pip install spacy\n",
        "!pip install catboost\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# Data manipulation and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesClassifier, BaggingClassifier, HistGradientBoostingClassifier,\n",
        "    GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier,\n",
        "    VotingClassifier, StackingClassifier, IsolationForest\n",
        ")\n",
        "from sklearn.linear_model import (\n",
        "    RidgeClassifier, PassiveAggressiveClassifier, Perceptron, SGDClassifier,\n",
        "    LogisticRegression, LogisticRegressionCV\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "\n",
        "# Gradient boosting frameworks\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# NLP libraries\n",
        "import gensim.downloader as api\n",
        "import spacy"
      ],
      "metadata": {
        "id": "DRnx-WaSekXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "nlp = spacy.load(\"en_core_web_lg\") # if this fails then run \"python -m spacy download en_core_web_lg\" to download that model\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZBIvBCiaekfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/kaggle/input/fake-news-detection-dataset-with-pre-trained-model/Fake News Classification using Machine Learning/Fake News Classification using Machine Learning/Training Dataset/Training.csv\")\n",
        "\n",
        "# Map 'label' to numerical values\n",
        "df['label_num'] = df['label'].map({'Fake': 0, 'Real': 1})\n",
        "\n",
        "# Remove all NaN values\n",
        "df = df.dropna()\n",
        "\n",
        "# Print the shape of the dataframe after removing NaN values\n",
        "print(\"Shape of the dataframe after removing NaN values:\", df.shape)\n",
        "\n",
        "\n",
        "# Check the distribution of labels\n",
        "print(\"Distribution of labels:\")\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "J_xhtsw8ekhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print top 5 rows\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "w1YR1YJNekkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving The vector\n",
        "#df.to_csv('Training2.csv', index=False)"
      ],
      "metadata": {
        "id": "dZREVB5ueknU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the distribution of labels\n",
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "1VoEvekmekq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def preprocess_and_vectorize(text):\n",
        "    # remove stop words and lemmatize the text\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "\n",
        "    return wv.get_mean_vector(filtered_tokens)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lIyH8KSjhiAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  # Take 30% of the data\n",
        "sampled_df = df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Print the shape of the sampled dataframe\n",
        "print(\"Shape of the sampled dataframe:\", sampled_df.shape)\n",
        "\n",
        "# Check the distribution of labels in the sampled dataframe\n",
        "print(\"Distribution of labels in the sampled dataframe:\")\n",
        "print(sampled_df['label'].value_counts())\n",
        "small_test_df =sampled_df\n",
        "# Measuring the time taken to convert the text to vectors\n",
        "start_time = time.time()\n",
        "\n",
        "# Converting Text to Vector\n",
        "small_test_df['vector'] = small_test_df['Text'].apply(lambda text: preprocess_and_vectorize(text))\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "# Convert elapsed time to hours, minutes, and seconds\n",
        "hours, rem = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "\n",
        "print(f\"Time taken to convert text to vectors: {int(hours)} hours, {int(minutes)} minutes, and {seconds:.2f} seconds\")\n",
        "\n",
        "def create_test_data(df, text_column, vector_column, label_column):\n",
        "\n",
        "\n",
        "    # Create X_test_2d and y_test\n",
        "    X_test_2d = np.stack(df[vector_column].values)\n",
        "    y_test = df[label_column].values\n",
        "\n",
        "    print(\"Shape of X_test after reshaping: \", X_test_2d.shape)\n",
        "    print(\"Shape of y_test: \", y_test.shape)\n",
        "\n",
        "    return X_test_2d, y_test\n",
        "\n",
        "X_test_2d, y_test = create_test_data(small_test_df, 'Text', 'vector', 'label_num')\n",
        "print(X_test_2d)\n",
        "print(y_test)\n",
        "\n",
        "small_test_df.to_csv('vector.csv', index=False)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I-fkiHg_hlIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Measuring the time taken to convert the text to vectors\n",
        "start_time = time.time()\n",
        "\n",
        "# Converting Text to Vector\n",
        "df['vector'] = df['Text'].apply(lambda text: preprocess_and_vectorize(text))\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "# Convert elapsed time to hours, minutes, and seconds\n",
        "hours, rem = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "\n",
        "print(f\"Time taken to convert text to vectors: {int(hours)} hours, {int(minutes)} minutes, and {seconds:.2f} seconds\")\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UpcbjfzZh0FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "def create_test_data(df, text_column, vector_column, label_column):\n",
        "\n",
        "\n",
        "    # Create X_test_2d and y_test\n",
        "    X_test_2d = np.stack(df[vector_column].values)\n",
        "    y_test = df[label_column].values\n",
        "\n",
        "    print(\"Shape of X_test after reshaping: \", X_test_2d.shape)\n",
        "    print(\"Shape of y_test: \", y_test.shape)\n",
        "\n",
        "    return X_test_2d, y_test\n",
        "\n",
        "X_DF_2d, y_DF = create_test_data(df, 'Text', 'vector', 'label_num')\n",
        "print(X_DF_2d)\n",
        "print(y_DF)\n",
        "\n",
        "#small_test_df.to_csv('vector.csv', index=False)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Create a dictionary to save the vectors and labels\n",
        "vector_data = {\n",
        "    'vector': X_DF_2d,\n",
        "    'label_num': y_DF\n",
        "}\n",
        "\n",
        "# Save the dictionary as a pickle file\n",
        "with open('3_8_0_vector_data.pkl', 'wb') as f:\n",
        "    pickle.dump(vector_data, f)\n",
        "\n",
        "print(\"Vector data saved as 'vector_data.pkl'\")\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qugq-pNzhp-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the vector data\n",
        "vector_file_path = \"/kaggle/input/fake-news-detection-dataset-with-pre-trained-model/spacy-models_de_core_news_lg-3.8.0/spacy-models_de_core_news_lg-3.8.0/3_8_0_vector_data.pkl\"\n",
        "\n",
        "with open(vector_file_path, 'rb') as f:\n",
        "    vector_data = pickle.load(f)\n",
        "\n",
        "X_DF_2d = vector_data['vector']\n",
        "y_DF = vector_data['label_num']\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "loaded_vector_df = pd.DataFrame(X_DF_2d)\n",
        "loaded_vector_df['label_num'] = y_DF\n",
        "\n",
        "#df = loaded_vector_df\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(loaded_vector_df.head())\n",
        "\n",
        "# Create the X (features) and y (label) for train-test split\n",
        "X = loaded_vector_df.drop(columns=['label_num']).values\n",
        "y = loaded_vector_df['label_num'].values\n",
        "\n",
        "# Perform the train-test split with test size of 20% with random state of 2022 and stratified sampling\n",
        "X_train_2d, X_test_2d, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.1,  # 10% samples will go to test dataset\n",
        "    random_state=2022,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train and test data shapes:\")\n",
        "print(\"X_train_2d shape:\", X_train_2d.shape)\n",
        "print(\"X_test_2d shape:\", X_test_2d.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "JGTePWAfh79Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bkH_XC7GiBLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}